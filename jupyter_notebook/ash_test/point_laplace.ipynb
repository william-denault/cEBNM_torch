{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Add the path to utils.py\n",
    "sys.path.append(r\"D:\\Document\\Serieux\\Travail\\python_work\\cEBNM_torch\\py\")\n",
    "from distribution_operation import *\n",
    "from utils import *\n",
    "from numerical_routine import *\n",
    "from posterior_computation import *\n",
    "from ash import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-16.91767637608251\n",
      "[7.62378256e-01 1.85932222e-11 1.85942848e-11 1.85964577e-11\n",
      " 1.86010255e-11 1.86114299e-11 1.86410570e-11 1.87732211e-11\n",
      " 1.96474741e-11 2.49075205e-11 4.52087257e-11 9.75060667e-11\n",
      " 2.30335211e-10 5.25821675e-06 2.36980217e-01 6.36267815e-04\n",
      " 3.90494875e-10 1.59697715e-10]\n",
      "[0.         0.03827328 0.05412659 0.07654655 0.10825318 0.15309311\n",
      " 0.21650635 0.30618622 0.4330127  0.61237244 0.8660254  1.22474487\n",
      " 1.73205081 2.44948974 3.46410162 4.89897949 6.92820323 9.79795897]\n"
     ]
    }
   ],
   "source": [
    "betahat=  np.array([1,2,3,4,5])\n",
    "sebetahat=np.array([1,0.4,5,1,1])\n",
    "\n",
    "res= ash(betahat, sebetahat, mult=np.sqrt(2))\n",
    "print(res.log_lik)\n",
    "print(res.pi )\n",
    "print(res.scale     )\n",
    "mult=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-1.5964616012204798)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "def logg_laplace(x, s, a):\n",
    "    \"\"\"\n",
    "    Compute the log of g, Laplace(a) convolved with a normal distribution.\n",
    "    \n",
    "    Args:\n",
    "        x (np.ndarray): Input data.\n",
    "        s (float or np.ndarray): Standard deviation of the normal distribution.\n",
    "        a (float): Scale parameter of the Laplace distribution.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Log-Laplace density values.\n",
    "    \"\"\"\n",
    "    lg1 = -a * x + norm.logcdf((x - s**2 * a) / s)\n",
    "    lg2 = a * x + norm.logcdf(-(x + s**2 * a) / s)  # Upper tail as -x\n",
    "    lfac = np.maximum(lg1, lg2)\n",
    "    return np.log(a / 2) + s**2 * a**2 / 2 + lfac + np.log(np.exp(lg1 - lfac) + np.exp(lg2 - lfac))\n",
    "\n",
    "\n",
    "logg_laplace(1,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-1.5037659241189736)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the path to utils.py\n",
    "sys.path.append(r\"D:\\Document\\Serieux\\Travail\\python_work\\cEBNM_torch\\py\")\n",
    "from numerical_routine import *\n",
    "# Function to compute the log likelihood under the point-laplace prior\n",
    "def loglik_point_laplace(x, s, w, a, mu):\n",
    "    \"\"\"\n",
    "    Compute the log likelihood under the point-laplace prior.\n",
    "    Args:\n",
    "        x: Observed data (array-like).\n",
    "        s: Standard deviation (scalar or array-like).\n",
    "        w: Weight for the mixture component (scalar).\n",
    "        a: Laplace scale parameter (scalar).\n",
    "        mu: Mean (scalar).\n",
    "    Returns:\n",
    "        Log likelihood (scalar).\n",
    "    \"\"\"\n",
    "    return np.sum(vloglik_point_laplace(x, s, w, a, mu))\n",
    "\n",
    "# Helper function to compute log((1 - w)f + wg) as a vector\n",
    "def vloglik_point_laplace(x, s, w, a, mu):\n",
    "    \"\"\"\n",
    "    Compute the vectorized log likelihood under the point-laplace prior.\n",
    "    Args:\n",
    "        x: Observed data (array-like).\n",
    "        s: Standard deviation (scalar or array-like).\n",
    "        w: Weight for the mixture component (scalar).\n",
    "        a: Laplace scale parameter (scalar).\n",
    "        mu: Mean (scalar).\n",
    "    Returns:\n",
    "        Log likelihood vector (array-like).\n",
    "    \"\"\"\n",
    "    if w <= 0:\n",
    "        return norm.logpdf(x - mu, scale=s)\n",
    "\n",
    "    lg = logg_laplace(x - mu, s, a)\n",
    "    if w == 1:\n",
    "        return lg\n",
    "\n",
    "    lf = norm.logpdf(x - mu, scale=s)\n",
    "    lfac = np.maximum(lg, lf)\n",
    "    return lfac + np.log((1 - w) * np.exp(lf - lfac) + w * np.exp(lg - lfac))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vloglik_point_laplace(1,1,0.5,1,0)\n",
    "loglik_point_laplace(1,1,0.5,1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.45573542, 0.57074938, 0.93438632])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wpost_laplace(x, s, w, a):\n",
    "    \"\"\"\n",
    "    Compute the posterior weights for the Laplace component.\n",
    "    \n",
    "    Args:\n",
    "        x (np.ndarray): Input data.\n",
    "        s (float or np.ndarray): Standard deviation of the normal component.\n",
    "        w (float): Weight for the Laplace component (mixture proportion).\n",
    "        a (float): Scale parameter of the Laplace distribution.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Posterior weights for the Laplace component.\n",
    "    \"\"\"\n",
    "    if w == 0:\n",
    "        return np.zeros_like(x)\n",
    "\n",
    "    if w == 1:\n",
    "        return np.ones_like(x)\n",
    "\n",
    "    # Log-density for the normal component\n",
    "    lf = norm.logpdf(x, loc=0, scale=s)\n",
    "\n",
    "    # Log-density for the Laplace component convolved with normal\n",
    "    lg = logg_laplace(x, s, a)\n",
    "\n",
    "    # Compute posterior weights\n",
    "    wpost = w / (w + (1 - w) * np.exp(lf - lg))\n",
    "    \n",
    "    return wpost\n",
    "\n",
    "x = np.array([1.0, 2.0, -1.5])\n",
    "s = np.array([1.0, 1.2, .5])\n",
    "wpost_laplace(x,s,0.5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.08054445392157272)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lambda_func(x, s, a):\n",
    "    \"\"\"\n",
    "    Compute lambda, the probability of being positive given a non-zero effect.\n",
    "    \n",
    "    Args:\n",
    "        x (np.ndarray): Input data.\n",
    "        s (np.ndarray): Standard deviations.\n",
    "        a (float): Laplace scale parameter.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Lambda values.\n",
    "    \"\"\"\n",
    "    lm1 = -a * x + norm.logcdf(x / s - s * a)\n",
    "    lm2 = a * x + norm.logcdf(-(x / s + s * a))\n",
    "    return 1 / (1 + np.exp(lm2 - lm1))\n",
    "\n",
    "\n",
    "lambda_func(-2,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66145273 0.75624598 0.68156531]\n",
      "[0.65301652 0.92274914 0.94291949]\n",
      "[ 0.33285794  0.73456863 -0.20122616]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Document\\Serieux\\Travail\\python_work\\cEBNM_torch\\py\\numerical_routine.py:48: RuntimeWarning: divide by zero encountered in divide\n",
      "  lower_bd = np.maximum(beta + 1 / beta, (alpha + beta) / 2)\n",
      "D:\\Document\\Serieux\\Travail\\python_work\\cEBNM_torch\\py\\numerical_routine.py:86: RuntimeWarning: invalid value encountered in multiply\n",
      "  alpha_frac = alpha * np.exp( np.clip(stats.norm.logpdf(alpha) - pnorm_diff, None, 700))\n",
      "D:\\Document\\Serieux\\Travail\\python_work\\cEBNM_torch\\py\\numerical_routine.py:87: RuntimeWarning: invalid value encountered in multiply\n",
      "  beta_frac = beta * np.exp( np.clip(stats.norm.logpdf(beta) - pnorm_diff, None, 700))\n",
      "D:\\Document\\Serieux\\Travail\\python_work\\cEBNM_torch\\py\\numerical_routine.py:105: RuntimeWarning: invalid value encountered in multiply\n",
      "  upper_bd2 = (alpha ** 2 + alpha * beta + beta ** 2) / 3\n",
      "D:\\Document\\Serieux\\Travail\\python_work\\cEBNM_torch\\py\\numerical_routine.py:105: RuntimeWarning: invalid value encountered in add\n",
      "  upper_bd2 = (alpha ** 2 + alpha * beta + beta ** 2) / 3\n"
     ]
    }
   ],
   "source": [
    "class PosteriorMeanPointLapalce:\n",
    "    def __init__(self, post_mean, post_mean2, post_sd):\n",
    "        self.post_mean = post_mean\n",
    "        self.post_mean2 = post_mean2\n",
    "        self.post_sd = post_sd\n",
    "\n",
    "def posterior_mean_laplace(x, s, w, a, mu=0):\n",
    "    \"\"\"\n",
    "    Compute the posterior mean for a normal mean under a Laplace prior.\n",
    "    \n",
    "    Args:\n",
    "        x (np.ndarray): Observed data.\n",
    "        s (float or np.ndarray): Standard deviation of the normal likelihood.\n",
    "        w (float): Mixture weight for the Laplace component.\n",
    "        a (float): Laplace scale parameter.\n",
    "        mu (float): Mean of the prior distribution (default is 0).\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Posterior means.\n",
    "    \"\"\"\n",
    "    # Compute posterior weights\n",
    "    wpost = wpost_laplace(x - mu, s, w, a)\n",
    "    print(wpost)\n",
    "    # Compute lambda (probability of being positive given non-null)\n",
    "    lm = lambda_func(x - mu, s, a)\n",
    "    \n",
    "    # Compute the truncated means for the Laplace component\n",
    "    laplace_mean_positive = my_etruncnorm(0, 99999 ,x - mu - s**2 * a, s)\n",
    "    laplace_mean_negative = my_etruncnorm(-99999, 0, x - mu + s**2 * a, s)\n",
    "    laplace_component_mean = lm * laplace_mean_positive + (1 - lm) * laplace_mean_negative\n",
    "    post_mean2              =  wpost * (lm * my_e2truncnorm(0, np.inf, x - mu - s**2 * a, s)\n",
    "                                       + (1 - lm) * my_e2truncnorm(-np.inf, 0, x - mu + s**2 * a, s))\n",
    "\n",
    "    \n",
    "    # Combine posterior means\n",
    "    post_mean =    wpost * laplace_component_mean  \n",
    "\n",
    "    if np.any(np.isinf(s)):\n",
    "        inf_indices = np.isinf(s)\n",
    "        a = 1/scale[1:]\n",
    "        # Equivalent of `post$mean[is.infinite(s)]` \n",
    "        post_mean[inf_indices] = wpost  / a \n",
    "\n",
    "        # Equivalent of `post$mean2[is.infinite(s)]`\n",
    "        post_mean2[inf_indices] =  2 * wpost / a**2 \n",
    "\n",
    "    post_mean2 = np.maximum(post_mean2, post_mean  ** 2)\n",
    "    \n",
    "    post_sd= np.sqrt(np.maximum(0, post_mean2 - post_mean**2))\n",
    "    post_mean2 = post_mean2 + mu**2+ 2*mu*post_mean\n",
    "    post_mean  =  post_mean+mu  \n",
    "    return PosteriorMeanPointLapalce(post_mean=post_mean,\n",
    "                                     post_mean2=post_mean2,\n",
    "                                     post_sd=  post_sd)\n",
    "\n",
    "\n",
    "\n",
    "x = np.array([1.0, 2.0, -1.5])\n",
    "s = np.array([1.0, 1.2, 2.5])\n",
    "a= posterior_mean_laplace(x,s,w=.7 ,a=1, mu=0 ) \n",
    "print(a.post_sd)\n",
    "print(a.post_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Log probability density function for the Laplace distribution\n",
    "def logg_laplace(x, s, a):\n",
    "    \"\"\"\n",
    "    Compute the log of the Laplace distribution.\n",
    "    Args:\n",
    "        x: Observed data (array-like).\n",
    "        s: Standard deviation (scalar or array-like).\n",
    "        a: Laplace scale parameter (scalar).\n",
    "    Returns:\n",
    "        Log Laplace density (array-like).\n",
    "    \"\"\"\n",
    "    b = s / np.sqrt(2 * a)\n",
    "    return -np.abs(x) / b - np.log(2 * b)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm, truncnorm\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "\n",
    "class LaplaceMixture:\n",
    "    def __init__(self, pi, mean, scale):\n",
    "        \"\"\"\n",
    "        Constructor for LaplaceMixture class.\n",
    "        \n",
    "        Args:\n",
    "            pi (np.ndarray): Mixture proportions.\n",
    "            mean (np.ndarray): Means of components.\n",
    "            scale (np.ndarray): Scale parameters of components.\n",
    "        \"\"\"\n",
    "        self.pi = np.asarray(pi)\n",
    "        self.mean = np.asarray(mean)\n",
    "        self.scale = np.asarray(scale)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"LaplaceMixture(pi={self.pi}, mean={self.mean}, scale={self.scale})\"\n",
    "def wpost_laplace(x, s, w, a):\n",
    "    \"\"\"\n",
    "    Compute posterior weights for non-null effects.\n",
    "    \n",
    "    Args:\n",
    "        x (np.ndarray): Input data.\n",
    "        s (float): Standard deviation.\n",
    "        w (float): Weight for the mixture component.\n",
    "        a (float): Scale parameter.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Posterior weights.\n",
    "    \"\"\"\n",
    "    if w == 0:\n",
    "        return np.zeros(len(x))\n",
    "    if w == 1:\n",
    "        return np.ones(len(x))\n",
    "\n",
    "    lf = norm.logpdf(x, loc=0, scale=s)\n",
    "    lg = logg_laplace(x, s, a)\n",
    "    return w / (w + (1 - w) * np.exp(lf - lg))\n",
    "\n",
    "\n",
    "def pl_nllik(par, x, s,  calc_grad=False):\n",
    "    \"\"\"\n",
    "    Compute the negative log likelihood and its gradient.\n",
    "    \n",
    "    Args:\n",
    "        par (dict): Parameters (alpha, beta, mu).\n",
    "        x (np.ndarray): Observed data.\n",
    "        s (np.ndarray): Standard deviation.\n",
    "        fix_par (list): Fixed parameters.\n",
    "        calc_grad (bool): Whether to calculate gradients.\n",
    "    \n",
    "    Returns:\n",
    "        float: Negative log likelihood.\n",
    "    \"\"\"\n",
    "    alpha, beta, mu = par['alpha'], par['beta'], par['mu']\n",
    "\n",
    "    # Convert alpha and beta to w and a\n",
    "    w = 1 - 1 / (1 + np.exp(alpha))\n",
    "    a = np.exp(beta)\n",
    "\n",
    "    lf = norm.logpdf(x - mu, scale=s)\n",
    "    lg = logg_laplace(x - mu, s, a)\n",
    "    llik = np.log((1 - w) * np.exp(lf) + w * np.exp(lg))\n",
    "    nllik = -np.sum(llik)\n",
    "\n",
    "    if calc_grad:\n",
    "        # Gradient calculations (optional)\n",
    "        pass\n",
    "\n",
    "    return nllik\n",
    "def logscale_add(logx, logy):\n",
    "    \"\"\"\n",
    "    Compute the log of the sum of exponentials of logx and logy.\n",
    "    \n",
    "    Args:\n",
    "        logx (float): Log value.\n",
    "        logy (float): Log value.\n",
    "    \n",
    "    Returns:\n",
    "        float: Log-sum-exp.\n",
    "    \"\"\"\n",
    "    max_log = np.maximum(logx, logy)\n",
    "    return max_log + np.log(np.exp(logx - max_log) + np.exp(logy - max_log))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def logscale_add(logx, logy):\n",
    "    \"\"\"\n",
    "    Compute log(exp(logx) + exp(logy)) in a numerically stable way.\n",
    "    \"\"\"\n",
    "    max_log = np.maximum(logx, logy)\n",
    "    return max_log + np.log(np.exp(logx - max_log) + np.exp(logy - max_log))\n",
    "\n",
    "def pl_nllik(par, x, s, par_init, fix_par, calc_grad=False, calc_hess=False):\n",
    "    \"\"\"\n",
    "    Compute the negative log likelihood and optionally its gradient and Hessian.\n",
    "\n",
    "    Args:\n",
    "        par (list): Parameters to optimize (subset based on fix_par).\n",
    "        x (np.ndarray): Observed data.\n",
    "        s (np.ndarray): Standard deviations.\n",
    "        par_init (list): Initial parameters (full set).\n",
    "        fix_par (list): Boolean list indicating which parameters to fix.\n",
    "        calc_grad (bool): Whether to calculate the gradient.\n",
    "        calc_hess (bool): Whether to calculate the Hessian.\n",
    "\n",
    "    Returns:\n",
    "        float: Negative log likelihood.\n",
    "        Optional: Gradient and Hessian as attributes of the output.\n",
    "    \"\"\"\n",
    "    fix_pi0, fix_a, fix_mu = fix_par\n",
    "\n",
    "    # Initialize parameters and update non-fixed values\n",
    "    p = np.array(par_init)\n",
    "    p[~np.array(fix_par)] = par\n",
    "\n",
    "    # Parameters\n",
    "    w = 1 - 1 / (1 + np.exp(p[0]))\n",
    "    a = np.exp(p[1])\n",
    "    mu = p[2]\n",
    "\n",
    "    # Point mass component\n",
    "    lf = -0.5 * np.log(2 * np.pi * s**2) - 0.5 * ((x - mu) / s)**2\n",
    "\n",
    "    # Laplace component: left tail\n",
    "    xleft = (x - mu) / s + s * a\n",
    "    lpnormleft = norm.logsf(xleft)\n",
    "    lgleft = np.log(a / 2) + s**2 * a**2 / 2 + a * (x - mu) + lpnormleft\n",
    "\n",
    "    # Laplace component: right tail\n",
    "    xright = (x - mu) / s - s * a\n",
    "    lpnormright = norm.logcdf(xright)\n",
    "    lgright = np.log(a / 2) + s**2 * a**2 / 2 - a * (x - mu) + lpnormright\n",
    "\n",
    "    # Combine left and right tails\n",
    "    lg = logscale_add(lgleft, lgright)\n",
    "\n",
    "    # Log likelihood\n",
    "    llik = logscale_add(np.log(1 - w) + lf, np.log(w) + lg)\n",
    "    nllik = -np.sum(llik)\n",
    "\n",
    "    # Gradients (optional)\n",
    "    if calc_grad or calc_hess:\n",
    "        grad = np.zeros(len(par))\n",
    "        i = 0\n",
    "\n",
    "        # Gradient with respect to w (alpha)\n",
    "        if not fix_pi0:\n",
    "            f = np.exp(lf - llik)\n",
    "            g = np.exp(lg - llik)\n",
    "            dnllik_dw = f - g\n",
    "            dw_dalpha = w * (1 - w)\n",
    "            grad[i] = np.sum(dnllik_dw * dw_dalpha)\n",
    "            i += 1\n",
    "\n",
    "        # Gradient with respect to a (beta)\n",
    "        if not fix_a:\n",
    "            dlogpnorm_left = -np.exp(-np.log(2 * np.pi) / 2 - xleft**2 / 2 - lpnormleft)\n",
    "            dlogpnorm_right = np.exp(-np.log(2 * np.pi) / 2 - xright**2 / 2 - lpnormright)\n",
    "\n",
    "            dgleft_da = np.exp(lgleft - llik) * (1 / a + a * s**2 + (x - mu) + s * dlogpnorm_left)\n",
    "            dgright_da = np.exp(lgright - llik) * (1 / a + a * s**2 - (x - mu) - s * dlogpnorm_right)\n",
    "            dg_da = dgleft_da + dgright_da\n",
    "\n",
    "            dnllik_da = -w * dg_da\n",
    "            da_dbeta = a\n",
    "            grad[i] = np.sum(dnllik_da * da_dbeta)\n",
    "            i += 1\n",
    "\n",
    "        # Gradient with respect to mu\n",
    "        if not fix_mu:\n",
    "            df_dmu = np.exp(lf - llik) * ((x - mu) / s**2)\n",
    "            dgleft_dmu = np.exp(lgleft - llik) * (-a - dlogpnorm_left / s)\n",
    "            dgright_dmu = np.exp(lgright - llik) * (a - dlogpnorm_right / s)\n",
    "            dg_dmu = dgleft_dmu + dgright_dmu\n",
    "            dnllik_dmu = -(1 - w) * df_dmu - w * dg_dmu\n",
    "            grad[i] = np.sum(dnllik_dmu)\n",
    "\n",
    "        nllik_grad = grad\n",
    "\n",
    "    # Hessian (optional)\n",
    "    if calc_hess:\n",
    "        hess = np.zeros((len(par), len(par)))\n",
    "        # The second derivatives would go here, similar to the gradient logic\n",
    "        # Implementation omitted for brevity\n",
    "\n",
    "        nllik_hess = hess\n",
    "\n",
    "    # Return results\n",
    "    if calc_grad and calc_hess:\n",
    "        return nllik, nllik_grad, nllik_hess\n",
    "    elif calc_grad:\n",
    "        return nllik, nllik_grad\n",
    "    else:\n",
    "        return nllik\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Log Likelihood: 5.74805874231258\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1.0, 2.0, -1.5])  # Observed data\n",
    "s = np.array([1.0, 1.0, 1.0])   # Standard deviations\n",
    "par_init = [0.0, 0.0, 0.0]      # Initial parameters (alpha, beta, mu)\n",
    "fix_par = [False, False, False] # No fixed parameters\n",
    "par = [0.5, 0.1, 1.0]           # Parameters to optimize\n",
    "\n",
    "# Compute negative log likelihood\n",
    "nllik = pl_nllik(par, x, s, par_init, fix_par, calc_grad=False)\n",
    "print(\"Negative Log Likelihood:\", nllik)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Parameters: [15.56216257  0.26743339  0.57725762]\n",
      "Final Negative Log Likelihood: 5.537673945869873\n"
     ]
    }
   ],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.04698323858334896)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "class optimizePointLalplace:\n",
    "    def __init__(self, w, a, mu ,nllik):\n",
    "        self.w = w\n",
    "        self.a = a\n",
    "        self.mu= mu\n",
    "        self.nllik= nllik\n",
    "\n",
    "def optimize_pl_nllik_with_gradient(x, s, par_init, fix_par):\n",
    "    \"\"\"\n",
    "    Optimize the negative log likelihood for the point-Laplace prior using gradient.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Observed data.\n",
    "        s (np.ndarray): Standard deviations.\n",
    "        par_init (list): Initial parameters [alpha, beta, mu].\n",
    "        fix_par (list): Boolean list indicating which parameters are fixed.\n",
    "\n",
    "    Returns:\n",
    "        dict: Optimized parameters and final negative log likelihood.\n",
    "    \"\"\"\n",
    "    # Define a wrapper for the objective function\n",
    "    def objective(par):\n",
    "        nllik, grad = pl_nllik(par, x, s, par_init, fix_par, calc_grad=True, calc_hess=False)\n",
    "        return nllik, grad\n",
    "\n",
    "    # Wrapper for gradient extraction\n",
    "    def fun(par):\n",
    "        nllik, _ = objective(par)\n",
    "        return nllik\n",
    "\n",
    "    def jac(par):\n",
    "        _, grad = objective(par)\n",
    "        return grad\n",
    "\n",
    "    # Initial values for non-fixed parameters\n",
    "    free_params = [p for p, fixed in zip(par_init, fix_par) if not fixed]\n",
    "\n",
    "    # Bounds (optional): Keep alpha unbounded, beta > 0, mu unbounded\n",
    "    bounds = [\n",
    "        (None, None),  # Alpha has no bounds\n",
    "        (None, None),  # Beta (transformed Laplace scale) has no explicit bounds\n",
    "        (None, None)   # Mu has no bounds\n",
    "    ]\n",
    "    bounds = [b for b, fixed in zip(bounds, fix_par) if not fixed]\n",
    "\n",
    "    # Optimize using L-BFGS-B\n",
    "    result = minimize(\n",
    "        fun, free_params, method=\"L-BFGS-B\", jac=jac, bounds=bounds\n",
    "    )\n",
    "\n",
    "    if not result.success:\n",
    "        raise ValueError(\"Optimization failed: \" + result.message)\n",
    "\n",
    "    # Update the full parameter set with optimized values\n",
    "    optimized_params = np.array(par_init)\n",
    "    optimized_params[~np.array(fix_par)] = result.x\n",
    "    optimized_params[0]= 1- 1/(1+ np.exp(optimized_params[0]))\n",
    "    \n",
    "    # Compute final negative log likelihood\n",
    "    \n",
    "\n",
    "    return  optimizePointLalplace(w=optimized_params[0],\n",
    "                                  a=optimized_params[1],\n",
    "                                  mu=optimized_params[2],\n",
    "                                  nllik= result.fun)\n",
    "# Example Usage\n",
    "x = np.array([0.0, 0.0, -0.5])  # Observed data\n",
    "s = np.array([1.0, 1.0, 1.0])   # Standard deviations\n",
    "par_init = [0.0, 0.0, 0.0]      # Initial parameters [alpha, beta, mu]\n",
    "fix_par = [False, False, False] # No fixed parameters\n",
    "\n",
    "# Run optimization\n",
    "result = optimize_pl_nllik_with_gradient(x, s, par_init, fix_par)\n",
    "result.nllik\n",
    "result.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ebnnm_point_laplace:\n",
    "    def __init__(self, post_mean, post_mean2, post_sd, scale, pi,   log_lik=0,#log_lik2 =0,\n",
    "                 mode=0):\n",
    "        self.post_mean = post_mean\n",
    "        self.post_mean2 = post_mean2\n",
    "        self.post_sd = post_sd\n",
    "        self.scale= scale\n",
    "        self.pi =pi  \n",
    "        self.log_lik = log_lik\n",
    "       # self.log_lik2= log_lik2 \n",
    "        self.mode =  mode\n",
    "\n",
    "\n",
    "def ebnnm_point_laplace_solver ( x,s,opt_mu=False,par_init = [0.0, 0.0, 0.0]  ):\n",
    "    if(opt_mu):\n",
    "        fix_par = [False, False, False]\n",
    "    else :\n",
    "        fix_par = [False, False, True]\n",
    "    par_init = par_init\n",
    "    optimized_prior =  optimize_pl_nllik_with_gradient(x, s, par_init, fix_par)\n",
    "    post_obj=  posterior_mean_laplace(x, s, optimized_prior.w, optimized_prior.a, optimized_prior.mu)\n",
    "    return( ebnnm_point_laplace( post_mean=post_obj.post_mean, \n",
    "                                post_mean2=post_obj.post_mean2, \n",
    "                                post_sd=post_obj.post_sd,\n",
    "                                scale=optimized_prior.a,\n",
    "                                pi=optimized_prior.w,   log_lik=-optimized_prior.nllik,#log_lik2 =0,\n",
    "                                mode=optimized_prior.mu)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99999986 1.         0.99999979]\n",
      "-4.161879692413249\n",
      "[ 0.          0.97913735 -0.33809281]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Document\\Serieux\\Travail\\python_work\\cEBNM_torch\\py\\numerical_routine.py:87: RuntimeWarning: invalid value encountered in multiply\n",
      "  beta_frac = beta * np.exp( np.clip(stats.norm.logpdf(beta) - pnorm_diff, None, 700))\n",
      "D:\\Document\\Serieux\\Travail\\python_work\\cEBNM_torch\\py\\numerical_routine.py:105: RuntimeWarning: invalid value encountered in add\n",
      "  upper_bd2 = (alpha ** 2 + alpha * beta + beta ** 2) / 3\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.0, 1.0, -0.5])  # Observed data\n",
    "s = np.array([1.0,  .2, 1.0])  \n",
    "ebnm_res= ebnnm_point_laplace_solver(x=x,s=s) \n",
    "\n",
    "\n",
    "print(ebnm_res.log_lik)\n",
    "\n",
    "print(ebnm_res.post_mean)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
