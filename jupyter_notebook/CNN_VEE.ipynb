{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "# Add the path to utils.py\n",
    "sys.path.append(r\"c:\\Document\\Serieux\\Travail\\python_work\\cEBNM_torch\\py\")\n",
    "\n",
    "# Import utils.py directly\n",
    "from utils import *\n",
    "from numerical_routine import *\n",
    "from distribution_operation import *\n",
    "from posterior_computation import *\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "batch_size=100\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "data_loader = DataLoader(mnist_data, batch_size=batch_size , shuffle=True)\n",
    "\n",
    "# Function to generate y values\n",
    "def generate_y(t, is_even):\n",
    "     \n",
    "    if is_even:        \n",
    "        if t <.5 :\n",
    "         return  np.random.normal(0, 2 )\n",
    "        else :\n",
    "            return np.zeros_like(t)\n",
    "    else:\n",
    "        if t>0.5:\n",
    "            return np.random.normal(0, 2 )\n",
    "        else :\n",
    "            return  np.zeros_like(t)\n",
    "        \n",
    "\n",
    "# Custom Dataset class to handle MNIST images and simulated data\n",
    "class SimulatedMNISTDataset(Dataset):\n",
    "    def __init__(self, mnist_data, positions, y_values, y_noisy, digits, noise_level, label_type, image_label):\n",
    "        self.image = mnist_data \n",
    "        self.mnist_data = mnist_data\n",
    "        self.positions = torch.tensor(positions, dtype=torch.float32)\n",
    "        self.y_values = torch.tensor(y_values, dtype=torch.float32)\n",
    "        self.y_noisy = torch.tensor(y_noisy, dtype=torch.float32)\n",
    "        self.digits = torch.tensor(digits, dtype=torch.long)\n",
    "        self.noise_level = noise_level\n",
    "        self.label_type = label_type\n",
    "        self.image_label=image_label\n",
    "        self.scale = autoselect_scales_mix_norm(\n",
    "            betahat=np.array(self.y_noisy),\n",
    "            sebetahat=np.array(self.noise_level)\n",
    "        )\n",
    "        \n",
    "        self.Lim =torch.tensor(  get_data_loglik_normal(\n",
    "            betahat=np.array(self.y_noisy),\n",
    "            sebetahat=np.array(self.noise_level),\n",
    "            location=0,\n",
    "            scale=self.scale\n",
    "                     ) , dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_data)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, _ = self.mnist_data[idx]\n",
    "        position = self.positions[idx]\n",
    "        y_value = self.y_values[idx]\n",
    "        y_noisy = self.y_noisy[idx]\n",
    "        digit = self.digits[idx]\n",
    "        marginal_log_lik = self.Lim[idx]\n",
    "        \n",
    "        return image, digit, position, y_value, y_noisy, marginal_log_lik\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "# Generate the simulated dataset\n",
    "n_samples = 10000\n",
    "# Generate the simulated dataset\n",
    "n_samples = 30000\n",
    "noise_level = 1\n",
    "positions = []\n",
    "y_values = []\n",
    "digits = []\n",
    "y_noisy = []\n",
    "label_type= []\n",
    "image_label =[]\n",
    "# Ensure unique indices are used\n",
    "unique_indices = np.random.choice(len(mnist_data), size=n_samples, replace=False)\n",
    "filtered_mnist_data = torch.utils.data.Subset(mnist_data, unique_indices)\n",
    "\n",
    "# Run simulation to generate noisy data\n",
    "for idx in unique_indices:\n",
    "    image, label = mnist_data[idx]\n",
    "    t = np.random.uniform(-0.5, 0.5)\n",
    "    is_even = label % 2 == 0\n",
    "    y = generate_y(t, is_even)\n",
    "    noise = np.random.normal(0, noise_level)\n",
    "    y_noisy_value = y + noise\n",
    "    positions.append(t)\n",
    "    y_values.append(y)\n",
    "    y_noisy.append(y_noisy_value)\n",
    "    digits.append(label)\n",
    "    image_label.append(label)\n",
    "    label_type.append(is_even)\n",
    "    \n",
    "positions = np.array(positions)\n",
    "y_values = np.array(y_values)\n",
    "y_noisy = np.array(y_noisy)\n",
    "digits = np.array(digits)\n",
    "label_type = np.array(label_type)\n",
    "image_label = np.array(image_label)\n",
    "noiseless=y_values\n",
    "# Create custom dataset with the filtered MNIST dataset and simulated data\n",
    "simulated_dataset = SimulatedMNISTDataset(\n",
    "    mnist_data=filtered_mnist_data,\n",
    "    positions=positions,\n",
    "    y_values=y_values,\n",
    "    y_noisy=y_noisy,\n",
    "    digits=digits,\n",
    "    noise_level=np.repeat(1, y_noisy.shape[0]),\n",
    "    label_type=label_type,\n",
    "    image_label =image_label\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21a4ca67bd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label =simulated_dataset.mnist_data[1]\n",
    "img\n",
    "plt.imshow(img.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.8117, -1.8098, -1.8044, -1.7851, -1.7360, -1.7200, -1.9573, -2.4743,\n",
      "        -3.1168, -3.7967])\n",
      "[-1.81170739 -1.80984514 -1.80442986 -1.78513494 -1.73603798 -1.71998596\n",
      " -1.95732894 -2.47431825 -3.11676061 -3.79673821]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "L_im = simulated_dataset.Lim\n",
    "print(L_im [0])\n",
    "scale= autoselect_scales_mix_norm(betahat=np.array(y_noisy),\n",
    "                             sebetahat=np.repeat(1, y_noisy.shape[0]))\n",
    "print(get_data_loglik_normal(y_noisy,  location=0,\n",
    "                             sebetahat=np.repeat(1, y_noisy.shape[0]),\n",
    "                             scale=scale)[0,])\n",
    "      \n",
    " \n",
    "n_epoch = 20\n",
    "batch_size =128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 1.901632\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 1.624126\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 1.653560\n",
      "====> Epoch: 1 Average loss: 1.6411\n",
      "Train Epoch: 2 [0/30000 (0%)]\tLoss: 1.670233\n",
      "Train Epoch: 2 [12800/30000 (43%)]\tLoss: 1.713978\n",
      "Train Epoch: 2 [25600/30000 (85%)]\tLoss: 1.690665\n",
      "====> Epoch: 2 Average loss: 1.6324\n",
      "Train Epoch: 3 [0/30000 (0%)]\tLoss: 1.657821\n",
      "Train Epoch: 3 [12800/30000 (43%)]\tLoss: 1.638749\n",
      "Train Epoch: 3 [25600/30000 (85%)]\tLoss: 1.569464\n",
      "====> Epoch: 3 Average loss: 1.6319\n",
      "Train Epoch: 4 [0/30000 (0%)]\tLoss: 1.559610\n",
      "Train Epoch: 4 [12800/30000 (43%)]\tLoss: 1.409208\n",
      "Train Epoch: 4 [25600/30000 (85%)]\tLoss: 1.468545\n",
      "====> Epoch: 4 Average loss: 1.6315\n",
      "Train Epoch: 5 [0/30000 (0%)]\tLoss: 1.674083\n",
      "Train Epoch: 5 [12800/30000 (43%)]\tLoss: 1.759804\n",
      "Train Epoch: 5 [25600/30000 (85%)]\tLoss: 1.601481\n",
      "====> Epoch: 5 Average loss: 1.6315\n",
      "Train Epoch: 6 [0/30000 (0%)]\tLoss: 1.658330\n",
      "Train Epoch: 6 [12800/30000 (43%)]\tLoss: 1.784972\n",
      "Train Epoch: 6 [25600/30000 (85%)]\tLoss: 1.779078\n",
      "====> Epoch: 6 Average loss: 1.6316\n",
      "Train Epoch: 7 [0/30000 (0%)]\tLoss: 1.626947\n",
      "Train Epoch: 7 [12800/30000 (43%)]\tLoss: 1.591707\n",
      "Train Epoch: 7 [25600/30000 (85%)]\tLoss: 1.653325\n",
      "====> Epoch: 7 Average loss: 1.6310\n",
      "Train Epoch: 8 [0/30000 (0%)]\tLoss: 1.552543\n",
      "Train Epoch: 8 [12800/30000 (43%)]\tLoss: 1.551901\n",
      "Train Epoch: 8 [25600/30000 (85%)]\tLoss: 1.731336\n",
      "====> Epoch: 8 Average loss: 1.6312\n",
      "Train Epoch: 9 [0/30000 (0%)]\tLoss: 1.711776\n",
      "Train Epoch: 9 [12800/30000 (43%)]\tLoss: 1.602475\n",
      "Train Epoch: 9 [25600/30000 (85%)]\tLoss: 1.669961\n",
      "====> Epoch: 9 Average loss: 1.6311\n",
      "Train Epoch: 10 [0/30000 (0%)]\tLoss: 1.574635\n",
      "Train Epoch: 10 [12800/30000 (43%)]\tLoss: 1.676590\n",
      "Train Epoch: 10 [25600/30000 (85%)]\tLoss: 1.834939\n",
      "====> Epoch: 10 Average loss: 1.6307\n",
      "Train Epoch: 11 [0/30000 (0%)]\tLoss: 1.645531\n",
      "Train Epoch: 11 [12800/30000 (43%)]\tLoss: 1.518691\n",
      "Train Epoch: 11 [25600/30000 (85%)]\tLoss: 1.699299\n",
      "====> Epoch: 11 Average loss: 1.6309\n",
      "Train Epoch: 12 [0/30000 (0%)]\tLoss: 1.517711\n",
      "Train Epoch: 12 [12800/30000 (43%)]\tLoss: 1.670520\n",
      "Train Epoch: 12 [25600/30000 (85%)]\tLoss: 1.662859\n",
      "====> Epoch: 12 Average loss: 1.6309\n",
      "Train Epoch: 13 [0/30000 (0%)]\tLoss: 1.587977\n",
      "Train Epoch: 13 [12800/30000 (43%)]\tLoss: 1.632822\n",
      "Train Epoch: 13 [25600/30000 (85%)]\tLoss: 1.682494\n",
      "====> Epoch: 13 Average loss: 1.6303\n",
      "Train Epoch: 14 [0/30000 (0%)]\tLoss: 1.707393\n",
      "Train Epoch: 14 [12800/30000 (43%)]\tLoss: 1.668582\n",
      "Train Epoch: 14 [25600/30000 (85%)]\tLoss: 1.567572\n",
      "====> Epoch: 14 Average loss: 1.6303\n",
      "Train Epoch: 15 [0/30000 (0%)]\tLoss: 1.555398\n",
      "Train Epoch: 15 [12800/30000 (43%)]\tLoss: 1.782102\n",
      "Train Epoch: 15 [25600/30000 (85%)]\tLoss: 1.739445\n",
      "====> Epoch: 15 Average loss: 1.6306\n",
      "Train Epoch: 16 [0/30000 (0%)]\tLoss: 1.669610\n",
      "Train Epoch: 16 [12800/30000 (43%)]\tLoss: 1.516692\n",
      "Train Epoch: 16 [25600/30000 (85%)]\tLoss: 1.668773\n",
      "====> Epoch: 16 Average loss: 1.6306\n",
      "Train Epoch: 17 [0/30000 (0%)]\tLoss: 1.652359\n",
      "Train Epoch: 17 [12800/30000 (43%)]\tLoss: 1.821747\n",
      "Train Epoch: 17 [25600/30000 (85%)]\tLoss: 1.754456\n",
      "====> Epoch: 17 Average loss: 1.6306\n",
      "Train Epoch: 18 [0/30000 (0%)]\tLoss: 1.656806\n",
      "Train Epoch: 18 [12800/30000 (43%)]\tLoss: 1.540496\n",
      "Train Epoch: 18 [25600/30000 (85%)]\tLoss: 1.685181\n",
      "====> Epoch: 18 Average loss: 1.6303\n",
      "Train Epoch: 19 [0/30000 (0%)]\tLoss: 1.672757\n",
      "Train Epoch: 19 [12800/30000 (43%)]\tLoss: 1.706362\n",
      "Train Epoch: 19 [25600/30000 (85%)]\tLoss: 1.602708\n",
      "====> Epoch: 19 Average loss: 1.6303\n",
      "Train Epoch: 20 [0/30000 (0%)]\tLoss: 1.652376\n",
      "Train Epoch: 20 [12800/30000 (43%)]\tLoss: 1.547380\n",
      "Train Epoch: 20 [25600/30000 (85%)]\tLoss: 1.733743\n",
      "====> Epoch: 20 Average loss: 1.6304\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Helper function to calculate the size after the convolutional layers\n",
    "def calculate_flattened_size(encoder, input_shape):\n",
    "    \"\"\"\n",
    "    Calculate the flattened size after passing an input through the encoder CNN.\n",
    "    Args:\n",
    "        encoder: The convolutional encoder network.\n",
    "        input_shape: Shape of the input (batch_size, channels, height, width).\n",
    "    Returns:\n",
    "        The flattened size after the convolutional layers.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        x = torch.rand(input_shape)  # Create a dummy input\n",
    "        x = encoder(x)  # Pass it through the encoder\n",
    "        return x.view(x.size(0), -1).shape[1]  # Return the flattened size\n",
    "\n",
    "# CNN-VAE with output as a set of fitted pi values\n",
    "class CNN_VAE_FittedPi(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, pi_dim):\n",
    "        super(CNN_VAE_FittedPi, self).__init__()\n",
    "\n",
    "        # Encoder: convolutional layers to encode the input image\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),  # 28x28 -> 14x14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # 14x14 -> 7x7\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # 7x7 -> 4x4\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Dynamically calculate the flattened size after the encoder\n",
    "        flattened_size = calculate_flattened_size(self.encoder, (1, 1, 28, 28))\n",
    "\n",
    "        # Fully connected layers for mean and log variance of latent space\n",
    "        self.fc_mu = nn.Linear(flattened_size, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(flattened_size, latent_dim)\n",
    "\n",
    "        # Decoder: fully connected layers (with positional input) to output fitted pi values\n",
    "        self.fc_decode = nn.Sequential(\n",
    "            nn.Linear(latent_dim + 1, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, pi_dim)  # Output matches the number of pi components\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, pos):\n",
    "        # Concatenate position to the latent vector\n",
    "        z = torch.cat([z, pos], dim=1)\n",
    "        x = self.fc_decode(z)\n",
    "        # Apply softmax to ensure the output sums to 1 (fitted pi)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def forward(self, x, pos):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        fitted_pi = self.decode(z, pos)\n",
    "        return fitted_pi, mu, logvar\n",
    "\n",
    "# Loss function for the penalized log-likelihood\n",
    "def vae_loss_function(pred_pi, marginal_log_lik, mu, logvar, penalty=10, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    Compute the penalized log likelihood function.\n",
    "    \n",
    "    Parameters:\n",
    "    pred_pi (torch.Tensor): A tensor of shape (batch_size, K) corresponding to pi_k for each sample.\n",
    "    marginal_log_lik (torch.Tensor): A tensor of shape (batch_size, K) corresponding to the log-likelihoods.\n",
    "    penalty (float): The penalty term for pi[:, 0].\n",
    "    epsilon (float): Small constant to avoid log of zero or division by zero.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The total loss (negative penalized log likelihood + KL divergence).\n",
    "    \"\"\"\n",
    "    # Reconstruction loss (penalized log-likelihood)\n",
    "    L_batch = torch.exp(marginal_log_lik)\n",
    "    inner_sum = torch.sum(pred_pi * L_batch, dim=1)\n",
    "    inner_sum = torch.clamp(inner_sum, min=epsilon)\n",
    "    recon_loss = torch.sum(torch.log(inner_sum))\n",
    "\n",
    "    # Penalty term on the first component of pred_pi\n",
    "    pi_clamped = torch.clamp(torch.sum(pred_pi[:, 0]), min=epsilon)\n",
    "    penalized_log_likelihood_value = recon_loss + (penalty - 1) * torch.log(pi_clamped)\n",
    "\n",
    "    # KL divergence (Kullback-Leibler Divergence)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    # Return the negative of the penalized log likelihood and the KL divergence\n",
    "    return -penalized_log_likelihood_value + kl_loss\n",
    "\n",
    "# Training function\n",
    "def train_vae(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, digit, pos, y_value, y_noisy, marginal_log_lik) in enumerate(train_loader):\n",
    "        data, pos, marginal_log_lik = data.to(device), pos.to(device).unsqueeze(1), marginal_log_lik.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred_pi, mu, logvar = model(data, pos)\n",
    "        loss = vae_loss_function(pred_pi, marginal_log_lik, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item() / len(data):.6f}')\n",
    "    \n",
    "    print(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_loader.dataset):.4f}')\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 784  # MNIST images (28x28)\n",
    "latent_dim = 20\n",
    "hidden_dim = 400\n",
    "pi_dim = simulated_dataset.Lim.size()[1]  # Number of components for fitted pi\n",
    "n_epoch = 20\n",
    "batch_size = 128\n",
    "\n",
    "# Main training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN_VAE_FittedPi(input_dim, hidden_dim, latent_dim, pi_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_loader = DataLoader(simulated_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(1, n_epoch + 1):\n",
    "    train_vae(model, device, train_loader, optimizer, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MLP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Main training loop\u001b[39;00m\n\u001b[0;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m MLP(input_dim, hidden_dim, simulated_dataset\u001b[38;5;241m.\u001b[39mLim\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m      8\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(simulated_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size , shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MLP' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_dim = 784  # MNIST images (28x28)\n",
    "hidden_dim = 400\n",
    "# Main training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MLP(input_dim, hidden_dim, simulated_dataset.Lim.size()[1]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_loader = DataLoader(simulated_dataset, batch_size=batch_size , shuffle=True)\n",
    "\n",
    "for epoch in range(1, n_epoch + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
