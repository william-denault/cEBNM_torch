{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math \n",
    "from scipy.stats import norm\n",
    "from scipy.stats import truncnorm\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoselect_scales_mix_norm(betahat, sebetahat, max_class=None, mult=2):\n",
    "    sigmaamin = np.min(sebetahat) / 10\n",
    "    if np.all(betahat**2 < sigmaamin**2):  # Fix the typo and ensure logical comparison\n",
    "        sigmaamax = 8 * sigmaamin\n",
    "    else:\n",
    "        sigmaamax = 2*np.sqrt(np.max(betahat**2 - sebetahat**2))\n",
    "    \n",
    "    if mult == 0:\n",
    "        out = np.array([0, sigmaamax / 2])\n",
    "    else:\n",
    "        npoint = math.ceil(math.log2(sigmaamax / sigmaamin) / math.log2(mult))\n",
    "\n",
    "        # Generate the sequence (-npoint):0 using np.arange\n",
    "        sequence = np.arange(-npoint, 1)\n",
    "\n",
    "        # Calculate the output\n",
    "        out = np.concatenate(([0], (1/mult) ** (-sequence) * sigmaamax))\n",
    "        if max_class!=None:\n",
    "            # Check if the length of out is equal to max_class\n",
    "            if len(out) != max_class:\n",
    "            # Generate a sequence from min(out) to max(out) with length max_class\n",
    "                out = np.linspace(np.min(out), np.max(out), num=max_class)\n",
    "        \n",
    "    \n",
    "    return out\n",
    "     \n",
    "def autoselect_scales_mix_exp(betahat, sebetahat, max_class=None , mult=1.5,tt=1.5):\n",
    "    sigmaamin = np.min(sebetahat) / 10\n",
    "    if np.all(betahat**2 < sigmaamin**2):  # Fix the typo and ensure logical comparison\n",
    "        sigmaamax = 8 * sigmaamin\n",
    "    else:\n",
    "        sigmaamax = tt*np.sqrt(np.max(betahat**2  ))\n",
    "    \n",
    "    if mult == 0:\n",
    "        out = np.array([0, sigmaamax / 2])\n",
    "    else:\n",
    "        npoint = math.ceil(math.log2(sigmaamax / sigmaamin) / math.log2(mult))\n",
    "\n",
    "        # Generate the sequence (-npoint):0 using np.arange\n",
    "        sequence = np.arange(-npoint, 1)\n",
    "\n",
    "        # Calculate the output\n",
    "        out = np.concatenate(([0], (1/mult) ** (-sequence) * sigmaamax))\n",
    "        if max_class!=None:\n",
    "            # Check if the length of out is equal to max_class\n",
    "            if len(out) != max_class:\n",
    "            # Generate a sequence from min(out) to max(out) with length max_class\n",
    "                out = np.linspace(np.min(out), np.max(out), num=max_class)\n",
    "                if(out[2] <1e-2 ):\n",
    "                 out[2: ] <- out[2: ] +1e-2\n",
    "         \n",
    "    \n",
    "    return out\n",
    "\n",
    "    \n",
    "def wpost_exp ( x, s, w, scale):\n",
    "    print(w)\n",
    "    if  w[0]==1:\n",
    "     out =  np.concatenate(([1]  ,np.full( scale.shape[0],[0])))\n",
    "     return out\n",
    "    else:\n",
    "     a=1/scale[1:]\n",
    "     w = assignment\n",
    "     a = 1 / scale[1:]  # Note: slicing in Python is zero-based, so [1:] starts from the second element\n",
    "     lf = norm.logpdf(x, loc=0, scale=s)\n",
    "     lg = np.log(a) + s**2 * a**2 / 2 - a * x + norm.logcdf(x / s - s * a)\n",
    "     log_prob = np.concatenate(([lf]  ,lg ))\n",
    "     bmax=np.max(log_prob)\n",
    "     log_prob = log_prob - bmax\n",
    " \n",
    "     log_prob = log_prob - bmax\n",
    "     wpost = w* np.exp( log_prob) / (sum(w *np.exp(log_prob)))\n",
    "     return wpost    \n",
    " \n",
    " \n",
    "\n",
    "def do_truncnorm_argchecks(a, b):\n",
    "    # Ensure a and b are numpy arrays, even if they are scalars\n",
    "    a = np.atleast_1d(np.asarray(a))\n",
    "    b = np.atleast_1d(np.asarray(b))\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError(\"truncnorm functions require that a and b have the same length.\")\n",
    "    if np.any(b < a):\n",
    "        raise ValueError(\"truncnorm functions require that a <= b.\")\n",
    "    return a, b\n",
    "\n",
    "def logscale_sub(logx, logy, epsilon=1e-10):\n",
    "    diff = logx - logy\n",
    "    if np.any(diff < 0):\n",
    "        bad_idx = diff < 0\n",
    "        logx[bad_idx] = logy[bad_idx]\n",
    "        print(f\"logscale_sub encountered negative value(s) of logx - logy (min: {np.min(diff[bad_idx]):.2e})\")\n",
    "    \n",
    "    scale_by = logx.copy()\n",
    "    scale_by[np.isinf(scale_by)] = 0\n",
    "    return np.log(np.exp(logx - scale_by) - np.exp(logy - scale_by) + epsilon) + scale_by\n",
    "\n",
    "def my_etruncnorm(a, b, mean=0, sd=1):\n",
    "    a, b = do_truncnorm_argchecks(a, b)\n",
    "    \n",
    "    alpha = (a - mean) / sd\n",
    "    beta = (b - mean) / sd\n",
    "    \n",
    "    flip = (alpha > 0) & (beta > 0) | (beta > np.abs(alpha))\n",
    "    orig_alpha = alpha.copy()\n",
    "    alpha[flip] = -beta[flip]\n",
    "    beta[flip] = -orig_alpha[flip]\n",
    "    \n",
    "    dnorm_diff = logscale_sub(stats.norm.logpdf(beta), stats.norm.logpdf(alpha))\n",
    "    pnorm_diff = logscale_sub(stats.norm.logcdf(beta), stats.norm.logcdf(alpha))\n",
    "    scaled_res = -np.exp(dnorm_diff - pnorm_diff)\n",
    "    \n",
    "    endpts_equal = np.isinf(pnorm_diff)\n",
    "    scaled_res[endpts_equal] = (alpha[endpts_equal] + beta[endpts_equal]) / 2\n",
    "    \n",
    "    lower_bd = np.maximum(beta + 1 / beta, (alpha + beta) / 2)\n",
    "    bad_idx = (~np.isnan(beta)) & (beta < 0) & ((scaled_res < lower_bd) | (scaled_res > beta))\n",
    "    scaled_res[bad_idx] = lower_bd[bad_idx]\n",
    "    \n",
    "    scaled_res[flip] = -scaled_res[flip]\n",
    "    \n",
    "    res = mean + sd * scaled_res\n",
    "    \n",
    "    if np.any(sd == 0):\n",
    "        a = np.tile(a, len(res))\n",
    "        b = np.tile(b, len(res))\n",
    "        mean = np.tile(mean, len(res))\n",
    "        \n",
    "        sd_zero = (sd == 0)\n",
    "        res[sd_zero & (b <= mean)] = b[sd_zero & (b <= mean)]\n",
    "        res[sd_zero & (a >= mean)] = a[sd_zero & (a >= mean)]\n",
    "        res[sd_zero & (a < mean) & (b > mean)] = mean[sd_zero & (a < mean) & (b > mean)]\n",
    "    \n",
    "    return res\n",
    "\n",
    "def my_e2truncnorm(a, b, mean=0, sd=1):\n",
    "    a, b = do_truncnorm_argchecks(a, b)\n",
    "    \n",
    "    mean = np.atleast_1d(mean)\n",
    "    sd   = np.atleast_1d(sd)\n",
    "    alpha = (a - mean) / sd\n",
    "    beta = (b - mean) / sd\n",
    "    \n",
    "    flip = (alpha > 0) & (beta > 0)\n",
    "    orig_alpha = alpha.copy()\n",
    "    alpha[flip] = -beta[flip]\n",
    "    beta[flip] = -orig_alpha[flip]\n",
    "    \n",
    "    if np.any(mean != 0):\n",
    "         \n",
    "        mean  =  abs(mean)\n",
    "    \n",
    "    pnorm_diff = logscale_sub(stats.norm.logcdf(beta), stats.norm.logcdf(alpha))\n",
    "    alpha_frac = alpha * np.exp(stats.norm.logpdf(alpha) - pnorm_diff)\n",
    "    beta_frac = beta * np.exp(stats.norm.logpdf(beta) - pnorm_diff)\n",
    "    \n",
    "    # Handle inf and nan values in alpha_frac and beta_frac\n",
    "    alpha_frac[np.isnan(alpha_frac) | np.isinf(alpha_frac)] = 0\n",
    "    beta_frac[np.isnan(beta_frac) | np.isinf(beta_frac)] = 0\n",
    "    \n",
    "    scaled_res = np.ones_like(alpha)\n",
    "    scaled_res[np.isnan(flip)] = np.nan\n",
    "    \n",
    "    alpha_idx = np.isfinite(alpha)\n",
    "    scaled_res[alpha_idx] = 1 + alpha_frac[alpha_idx]\n",
    "    beta_idx = np.isfinite(beta)\n",
    "    scaled_res[beta_idx] -= beta_frac[beta_idx]\n",
    "    \n",
    "    endpts_equal = np.isinf(pnorm_diff)\n",
    "    scaled_res[endpts_equal] = ((alpha[endpts_equal] + beta[endpts_equal]) ** 2) / 4\n",
    "    \n",
    "    upper_bd1 = beta ** 2 + 2 * (1 + 1 / beta ** 2)\n",
    "    upper_bd2 = (alpha ** 2 + alpha * beta + beta ** 2) / 3\n",
    "    upper_bd = np.minimum(upper_bd1, upper_bd2)\n",
    "    bad_idx = (~np.isnan(beta)) & (beta < 0) & ((scaled_res < beta ** 2) | (scaled_res > upper_bd))\n",
    "    scaled_res[bad_idx] = upper_bd[bad_idx]\n",
    "    \n",
    "    res = mean ** 2 + 2 * mean * sd * my_etruncnorm(alpha, beta) + sd ** 2 * scaled_res\n",
    "    \n",
    "    if np.any(sd == 0):\n",
    "        a = np.tile(a, len(res))\n",
    "        b = np.tile(b, len(res))\n",
    "        mean = np.tile(mean, len(res))\n",
    "        \n",
    "        sd_zero = (sd == 0)\n",
    "        res[sd_zero & (b <= mean)] = b[sd_zero & (b <= mean)] ** 2\n",
    "        res[sd_zero & (a >= mean)] = a[sd_zero & (a >= mean)] ** 2\n",
    "        res[sd_zero & (a < mean) & (b > mean)] = mean[sd_zero & (a < mean) & (b > mean)] ** 2\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "class PosteriorMeanExp:\n",
    "    def __init__(self, post_mean, post_mean2, post_sd):\n",
    "        self.post_mean = post_mean\n",
    "        self.post_mean2 = post_mean2\n",
    "        self.post_sd = post_sd\n",
    "\n",
    "def posterior_mean_exp(betahat, sebetahat, log_pi, scale):\n",
    "    assignment = np.exp(log_pi)\n",
    "    assignment = assignment / assignment.sum(axis=1, keepdims=True)\n",
    "    mu = 0\n",
    "    post_assign = np.zeros((betahat.shape[0], scale.shape[0]))\n",
    "    \n",
    "    for i in range(betahat.shape[0]):\n",
    "        post_assign[i,] = wpost_exp(x=betahat[i],\n",
    "                                    s=sebetahat[i], \n",
    "                                    w=assignment[i,],\n",
    "                                    scale=scale) \n",
    "    \n",
    "    post_mean = np.zeros(betahat.shape[0])\n",
    "    post_mean2 = np.zeros(betahat.shape[0])\n",
    "\n",
    "    for i in range(post_mean.shape[0]):\n",
    "        post_mean[i] = sum(post_assign[i, 1:] * my_etruncnorm(0,\n",
    "                                                              np.inf,\n",
    "                                                              betahat[i] - sebetahat[i]**2 * (1/scale[1:]), \n",
    "                                                              sebetahat[i]))\n",
    "        post_mean2[i] = sum(post_assign[i, 1:] * my_e2truncnorm(0,\n",
    "                                                                99999, #some weird warning for inf so just use something large enough for b\n",
    "                                                                betahat[i] - sebetahat[i]**2 * (1/scale[1:]), \n",
    "                                                                sebetahat[i]))\n",
    "        post_mean2[i] = max(post_mean[i], post_mean2[i])\n",
    "    \n",
    "    if np.any(np.isinf(sebetahat)):\n",
    "        inf_indices = np.isinf(sebetahat)\n",
    "        a = 1/scale[1:]\n",
    "        # Equivalent of `post$mean[is.infinite(s)]` \n",
    "        post_mean[inf_indices] = np.sum(post_assign[inf_indices, 1:] / a, axis=1)\n",
    "\n",
    "        # Equivalent of `post$mean2[is.infinite(s)]`\n",
    "        post_mean2[inf_indices] = np.sum(2 * post_assign[inf_indices, 1:] / a**2, axis=1)\n",
    "\n",
    "    # Calculate `post_sd`\n",
    "    post_sd = np.sqrt(np.maximum(0, post_mean2 - post_mean**2))\n",
    "\n",
    "    # Update `post_mean2` and `post_mean`\n",
    "    post_mean2 = post_mean2 + mu**2 + 2 * mu * post_mean\n",
    "    post_mean = post_mean + mu\n",
    "\n",
    "    # Return the results as an instance of PosteriorMeanExpResult\n",
    "    return PosteriorMeanExp(post_mean, post_mean2, post_sd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value of the truncated normal distribution: [1.48995049]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "expected_value =my_etruncnorm(0,2,3,1)\n",
    "\n",
    "print(f\"Expected value of the truncated normal distribution: {expected_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should equal to 1.48995049"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.39340536])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_e2truncnorm(0,2,3,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should equal to  2.393405"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "betahat=  np.array([1,2,3,4,5])\n",
    "sebetahat=np.array([1,0.4,5,1,1])\n",
    "scale = autoselect_scales_mix_exp ( np.array([1,2,3,4,5]),  np.array([1,0.4,5,1,1]))\n",
    "\n",
    "non_informativ = np.full( scale.shape[0], 1/ scale.shape[0])\n",
    "n=betahat.shape[0]\n",
    "log_pi =  np.log( np.tile(non_informativ, (n, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667]\n"
     ]
    }
   ],
   "source": [
    "assignment = np.exp(log_pi)[0]\n",
    "\n",
    "\n",
    "assignment = assignment /   sum(assignment)\n",
    "print(assignment)\n",
    "x=betahat[1]\n",
    "s=sebetahat[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writing wpost_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667]\n",
      "[0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667]\n",
      "[0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667]\n",
      "[3.53987758e-06 6.61493885e-06 1.06391083e-05 2.86976960e-05\n",
      " 1.69165759e-04 1.40776600e-03 8.91255655e-03 3.45101678e-02\n",
      " 8.34205613e-02 1.38160703e-01 1.72844260e-01 1.77093219e-01\n",
      " 1.57939387e-01 1.28091688e-01 9.74010338e-02]\n"
     ]
    }
   ],
   "source": [
    "w=assignment\n",
    "print(w)\n",
    "wpost_exp ( x, s, w, scale)\n",
    "temp_array =   np.zeros ( (betahat.shape[0], scale.shape[0]))\n",
    "i=1\n",
    "temp_array[i,] = wpost_exp ( x=betahat[i], s=sebetahat[i], w=w, scale=scale) \n",
    "print(temp_array[i,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should  be array([3.53987758e-06, 6.61493885e-06, 1.06391083e-05, 2.86976960e-05,\n",
    "       1.69165759e-04, 1.40776600e-03, 8.91255655e-03, 3.45101678e-02,\n",
    "       8.34205613e-02, 1.38160703e-01, 1.72844260e-01, 1.77093219e-01,\n",
    "       1.57939387e-01, 1.28091688e-01, 9.74010338e-02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "[0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667]\n",
      "[0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667]\n",
      "[0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667]\n",
      "[0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667]\n",
      "[0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667]\n",
      "[[6.52918851e-02 6.78002962e-02 6.90396965e-02 7.08629747e-02\n",
      "  7.34774073e-02 7.70129806e-02 8.12078860e-02 8.48923837e-02\n",
      "  8.58678052e-02 8.20342129e-02 7.30701767e-02 6.07291363e-02\n",
      "  4.75513198e-02 3.55270606e-02 2.56347785e-02]\n",
      " [3.53987758e-06 6.61493885e-06 1.06391083e-05 2.86976960e-05\n",
      "  1.69165759e-04 1.40776600e-03 8.91255655e-03 3.45101678e-02\n",
      "  8.34205613e-02 1.38160703e-01 1.72844260e-01 1.77093219e-01\n",
      "  1.57939387e-01 1.28091688e-01 9.74010338e-02]\n",
      " [6.56166419e-02 6.59175381e-02 6.60660268e-02 6.62862131e-02\n",
      "  6.66105355e-02 6.70828769e-02 6.77572351e-02 6.86850138e-02\n",
      "  6.98706687e-02 7.11576607e-02 7.20227505e-02 7.14207842e-02\n",
      "  6.80838604e-02 6.13953338e-02 5.20268604e-02]\n",
      " [2.97209370e-04 3.50647498e-04 3.84449298e-04 4.47398412e-04\n",
      "  5.81502557e-04 9.36473754e-04 2.17223266e-03 7.15579060e-03\n",
      "  2.44160538e-02 6.53144569e-02 1.26462752e-01 1.82759260e-01\n",
      "  2.09880744e-01 2.03590339e-01 1.75250690e-01]\n",
      " [4.80878262e-06 5.94302299e-06 6.71979846e-06 8.30282145e-06\n",
      "  1.23434606e-05 2.80212386e-05 1.35971200e-04 1.11185612e-03\n",
      "  7.83668609e-03 3.46083323e-02 9.38210739e-02 1.69756802e-01\n",
      "  2.26479200e-01 2.42787969e-01 2.23395970e-01]]\n",
      "[0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(scale.shape[0] )\n",
    "post_assign =   np.zeros ( (betahat.shape[0], scale.shape[0]))\n",
    "for i in range(betahat.shape[0]):\n",
    "    post_assign[i,] = wpost_exp ( x=betahat[i], s=sebetahat[i], w=np.exp(log_pi)[i,], scale=scale) \n",
    "print(post_assign)\n",
    "\n",
    "post_assign[0,]== wpost_exp ( x=betahat[0], s=sebetahat[0], w=w, scale=scale) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0.]\n",
      "[0.4836384  1.89328341 1.05670973 3.57009763 4.66379651]\n",
      "[1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "post_mean = np.zeros(betahat.shape[0])\n",
    "post_mean2 = np.zeros(betahat.shape[0])\n",
    "print(post_mean)\n",
    "\n",
    "for i in range(post_mean.shape[0]):\n",
    "    post_mean[i]=  sum(    post_assign[i,1:] *  my_etruncnorm(0,\n",
    "                                             np.inf,\n",
    "                                             betahat[i]- sebetahat[i]** 2 *(1/scale[1:]) , \n",
    "                                             sebetahat[i] \n",
    "                                             )\n",
    "                       )\n",
    "    post_mean2[i] =  sum ( post_assign[i,1:] *my_e2truncnorm(0,\n",
    "                                             99999, #some weird warning for inf so just use something large enought for b\n",
    "                                             betahat[i]- sebetahat[i]** 2 *(1/scale[1:]) , \n",
    "                                             sebetahat[i] \n",
    "                                             )\n",
    "                       )\n",
    "     \n",
    "    post_mean2[i]= max(post_mean[i],post_mean2[i])\n",
    "print(post_mean)\n",
    "\n",
    "print(betahat )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667]\n",
      "[0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667]\n",
      "[0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667]\n",
      "[0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667]\n",
      "[0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667]\n",
      "[0.4836384  1.89328341 1.05670973 3.57009763 4.66379651]\n",
      "[ 0.59674218  3.75372025  4.34337321 13.89299102 22.81107216]\n"
     ]
    }
   ],
   "source": [
    "n=betahat.shape[0]\n",
    "p= scale.shape[0]\n",
    " \n",
    "log_pi =  np.log( np.full( (n, p), 1/scale.shape[0]))\n",
    " \n",
    "res = posterior_mean_exp(betahat, sebetahat, log_pi, scale)\n",
    "print(res.post_mean)\n",
    "print(res.post_mean2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "res.post_mean should be equal to [0.4836384 1.8932834 1.0567097 3.5700976 4.6637965]\n",
    "res.post_mean2 should be equal to [ 0.5967422  3.7537202  4.3433782 13.8929910 22.8110722]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rscript to double check code aboce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "x  <- c( 1,2,3,4,5 )\n",
    "s  <- c( 1,0.4,5,1,1 )\n",
    " \n",
    "mu <- 0\n",
    "scale <-  autoselect_scales_mix_exp ( c(1,2,3,4,5 ),  c(  1,0.4,5,1,1 ))\n",
    "\n",
    "a=1/scale[-1]\n",
    "post <- list()\n",
    "\n",
    "\n",
    "wpost_exp <- function(x, s, w,g) {\n",
    "  \n",
    "  # assuming a[1 ]=0\n",
    "  if (w[1] == 1) {\n",
    "    \n",
    "    return(c(1, rep(0,length(g$scale) - 1))  )\n",
    "  }\n",
    "  \n",
    "  \n",
    "  lf <- dnorm(x, 0, s, log = TRUE)\n",
    "  lg <- log(a) + s^2 * a^2 / 2 - a * x + pnorm(x / s - s * a, log.p = TRUE)\n",
    "  \n",
    "  \n",
    "  \n",
    "  log_prob = c(lf, lg)\n",
    "  bmax = max(log_prob)\n",
    "  log_prob = log_prob - bmax\n",
    "  wpost <- w* exp( log_prob) / (sum(w *exp(log_prob)))\n",
    "  \n",
    "  #wpost <- w*c(exp(lf), exp( lg)) / (sum(w *c(exp(lf), exp( lg))))#underflow here\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  return(wpost)\n",
    "}\n",
    "\n",
    "assignment <- matrix(1/length(scale), nrow = length(x), ncol = length(scale))\n",
    "\n",
    "post_assignment <- do.call(rbind,\n",
    "               lapply(1:length(x),\n",
    "                      function(i)\n",
    "                        wpost_exp(x = x[i],\n",
    "                                  s = s[i],\n",
    "                                  w = assignment [i, ],\n",
    "                                  g =  fit$g)\n",
    "               )\n",
    ")\n",
    "\n",
    "post$mean  <- apply( post_assignment[,-1] *ashr:: my_etruncnorm(0,\n",
    "                                                                Inf,\n",
    "                                                                x - s^2 %*% t(a),\n",
    "                                                                s),\n",
    "                     1,\n",
    "                     sum)\n",
    "\n",
    "post$mean2 <- apply( post_assignment[,-1] *ashr:: my_e2truncnorm(0,\n",
    "                                                                 Inf,\n",
    "                                                                 x - s^2 %*% t(a),\n",
    "                                                                 s),\n",
    "                     1,\n",
    "                     sum)\n",
    "post$mean2 <- pmax(post$mean2, post$mean^2)\n",
    "\n",
    "\n",
    "\n",
    "if (any(is.infinite(s))) {\n",
    "  post$mean[is.infinite(s)]  <-  apply(\n",
    "    post_assignment[is.infinite(s), -1] / a,\n",
    "    1,\n",
    "    sum)\n",
    "  post$mean2[is.infinite(s)] <-  apply(\n",
    "    2*post_assignment[is.infinite(s), -1] / a^2,\n",
    "    1,\n",
    "    sum)\n",
    "}\n",
    "post$sd <- sqrt(pmax(0, post$mean2 - post$mean^2))\n",
    "\n",
    "post$mean2 <- post$mean2 + mu^2 + 2 * mu * post$mean\n",
    "post$mean  <- post$mean + mu\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
