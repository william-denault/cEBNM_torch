{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 11 iterations.\n",
      "Estimated π: 0.6153223121848359\n",
      "Estimated μ: 4.603705295377346\n",
      "Estimated τ^2: 4.520206597144137\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "class EMAlgorithmPointMassTruncated:\n",
    "    def __init__(self, x, sigma_0, tau_squared, pi, max_iter=100, tol=1e-6):\n",
    "        \"\"\"\n",
    "        EM algorithm for a mixture of a point mass at 0 and a Gaussian component.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Observed data points (1D array)\n",
    "        - sigma_0: Fixed noise standard deviation (scalar)\n",
    "        - tau_squared: Initial value for the variance of the Gaussian component\n",
    "        - pi: Initial value for the mixing proportion \\(\\pi\\)\n",
    "        - max_iter: Maximum number of iterations\n",
    "        - tol: Tolerance for convergence\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.sigma_0 = sigma_0\n",
    "        self.tau_squared = tau_squared\n",
    "        self.pi = pi\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.n = len(x)\n",
    "        self.mu = np.mean(x)  # Initialize \\(\\mu\\)\n",
    "\n",
    "    def e_step(self):\n",
    "        \"\"\"\n",
    "        E-step: Compute posterior probabilities \\(q_i\\) for each data point.\n",
    "        \"\"\"\n",
    "        sigma_2_total = self.sigma_0**2 + 1/self.tau_squared\n",
    "        phi_0 = (self.x == 0).astype(float)  # Point mass at 0\n",
    "        phi_2 = norm.pdf(self.x, self.mu, np.sqrt(sigma_2_total))\n",
    "        numerator = self.pi * phi_2\n",
    "        denominator = (1 - self.pi) * phi_0 + self.pi * phi_2\n",
    "        q = numerator / (denominator + 1e-8)  # Avoid division by zero\n",
    "        return q\n",
    "\n",
    "    def m_step(self, q):\n",
    "        \"\"\"\n",
    "        M-step: Update parameters \\(\\pi\\), \\(\\mu\\), and \\(\\tau^2\\).\n",
    "        \"\"\"\n",
    "        # Update \\(\\pi\\)\n",
    "        self.pi = np.mean(q)\n",
    "\n",
    "        # Update \\(\\mu\\) (Equation 23 in the document)\n",
    "        def objective(mu):\n",
    "            sigma_2_total = self.sigma_0**2 + 1/self.tau_squared\n",
    "            return -np.sum(q * norm.logpdf(self.x, mu, np.sqrt(sigma_2_total)))\n",
    "\n",
    "        res = minimize(objective, self.mu, method=\"L-BFGS-B\")\n",
    "        self.mu = res.x[0]\n",
    "\n",
    "        # Update \\(\\tau^2\\) (Equation 25 in the document)\n",
    "        self.tau_squared = np.sum(q * (self.x - self.mu) ** 2) / np.sum(q)\n",
    "        self.tau_squared = max(self.tau_squared, 1e-8)  # Ensure non-negative variance\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Run the EM algorithm until convergence.\n",
    "\n",
    "        Returns:\n",
    "        - pi: Estimated mixing proportion\n",
    "        - mu: Estimated mean of the Gaussian component\n",
    "        - tau_squared: Estimated variance of the Gaussian component\n",
    "        \"\"\"\n",
    "        for iteration in range(self.max_iter):\n",
    "            # E-step\n",
    "            q = self.e_step()\n",
    "\n",
    "            # Save old parameters to check for convergence\n",
    "            old_params = np.array([self.pi, self.mu, self.tau_squared])\n",
    "\n",
    "            # M-step\n",
    "            self.m_step(q)\n",
    "\n",
    "            # Check convergence\n",
    "            new_params = np.array([self.pi, self.mu, self.tau_squared])\n",
    "            if np.linalg.norm(new_params - old_params) < self.tol:\n",
    "                print(f\"Converged in {iteration + 1} iterations.\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"Maximum iterations reached without convergence.\")\n",
    "\n",
    "        return self.pi, self.mu, self.tau_squared\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data\n",
    " \n",
    "    point_mass = np.zeros(100)\n",
    "    gaussian_component = np.random.normal(5, 2, 150)\n",
    "    x = np.concatenate([point_mass, gaussian_component])\n",
    "\n",
    "    # EM algorithm parameters\n",
    "    sigma_0 = 1.0  # Fixed standard deviation for the point mass\n",
    "    tau_squared = 2.0  # Initial guess for variance of the Gaussian component\n",
    "    pi = 0.5  # Initial guess for mixing proportion\n",
    "\n",
    "    # Run the EM algorithm\n",
    "    em = EMAlgorithmPointMassTruncated(x, sigma_0, tau_squared, pi)\n",
    "    pi, mu, tau_squared = em.run()\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Estimated \\u03c0: {pi}\")\n",
    "    print(f\"Estimated \\u03bc: {mu}\")\n",
    "    print(f\"Estimated \\u03c4^2: {tau_squared}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 11 iterations.\n",
      "Estimated π: 0.6109283479236458\n",
      "Estimated μ: 4.748850083508237\n",
      "Estimated τ^2: 3.875994541791946\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "point_mass = np.zeros(100)\n",
    "gaussian_component = np.random.normal(5, 2, 150)\n",
    "x = np.concatenate([point_mass, gaussian_component])\n",
    "\n",
    "    # EM algorithm parameters\n",
    "sigma_0 = 1.0  # Fixed standard deviation for the point mass\n",
    "tau_squared = 2.0  # Initial guess for variance of the Gaussian component\n",
    "pi = 0.5  # Initial guess for mixing proportion\n",
    "\n",
    "    # Run the EM algorithm\n",
    "em = EMAlgorithmPointMassTruncated(x, sigma_0, tau_squared, pi)\n",
    "pi, mu, tau_squared = em.run()\n",
    "\n",
    "    # Print results\n",
    "print(f\"Estimated \\u03c0: {pi}\")\n",
    "print(f\"Estimated \\u03bc: {mu}\")\n",
    "print(f\"Estimated \\u03c4^2: {tau_squared}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached without convergence.\n",
      "Estimated μ: 4.741789097525298\n",
      "Estimated τ^2: 3.9037142696758833\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "class EMAlgorithmWithCovariates:\n",
    "    def __init__(self, x, z, sigma_0, tau_squared, max_iter=100, tol=1e-6, hidden_dim=16, lr=0.01):\n",
    "        \"\"\"\n",
    "        EM algorithm for a mixture of a point mass at 0 and a Gaussian component with covariate-dependent mixing proportion.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Observed data points (1D array)\n",
    "        - z: Covariate (1D array)\n",
    "        - sigma_0: Fixed noise standard deviation for the point mass\n",
    "        - tau_squared: Initial value for the variance of the Gaussian component\n",
    "        - max_iter: Maximum number of iterations\n",
    "        - tol: Tolerance for convergence\n",
    "        - hidden_dim: Hidden dimension size for the neural network\n",
    "        - lr: Learning rate for the neural network optimizer\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.z = torch.tensor(z, dtype=torch.float32).unsqueeze(1)  # Covariate\n",
    "        self.sigma_0 = sigma_0\n",
    "        self.tau_squared = tau_squared\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.n = len(x)\n",
    "        self.mu = np.mean(x)  # Initialize \\(\\mu\\)\n",
    "\n",
    "        # Neural network to model \\(\\pi(z)\\)\n",
    "        self.pi_net = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid(),  # Ensure output is in (0, 1)\n",
    "        )\n",
    "        self.optimizer = optim.Adam(self.pi_net.parameters(), lr=lr)\n",
    "\n",
    "    def e_step(self):\n",
    "        \"\"\"\n",
    "        E-step: Compute posterior probabilities \\(q_i\\) for each data point.\n",
    "        \"\"\"\n",
    "        # Predict \\(\\pi(z)\\) using the neural network\n",
    "        with torch.no_grad():\n",
    "            pi_z = self.pi_net(self.z).squeeze().numpy()\n",
    "\n",
    "        sigma_2_total = self.sigma_0**2 + self.tau_squared\n",
    "        phi_0 = (self.x == 0).astype(float)  # Point mass at 0\n",
    "        phi_2 = norm.pdf(self.x, self.mu, np.sqrt(sigma_2_total))\n",
    "        numerator = pi_z * phi_2\n",
    "        denominator = (1 - pi_z) * phi_0 + pi_z * phi_2\n",
    "        q = numerator / (denominator + 1e-8)  # Avoid division by zero\n",
    "        return q, pi_z\n",
    "\n",
    "    def m_step(self, q):\n",
    "        \"\"\"\n",
    "        M-step: Update parameters \\(\\mu\\), and \\(\\tau^2\\).\n",
    "        \"\"\"\n",
    "        # Update \\(\\mu\\) (optimize Equation 23)\n",
    "        def objective(mu):\n",
    "            sigma_2_total = self.sigma_0**2 + self.tau_squared\n",
    "            return -np.sum(q * norm.logpdf(self.x, mu, np.sqrt(sigma_2_total)))\n",
    "\n",
    "        res = minimize(objective, self.mu, method=\"L-BFGS-B\")\n",
    "        self.mu = res.x[0]\n",
    "\n",
    "        # Update \\(\\tau^2\\) (Equation 25)\n",
    "        self.tau_squared = np.sum(q * (self.x - self.mu) ** 2) / np.sum(q)\n",
    "        self.tau_squared = max(self.tau_squared, 1e-8)  # Ensure non-negative variance\n",
    "\n",
    "    def update_pi_net(self, q):\n",
    "        \"\"\"\n",
    "        Update the neural network \\(\\pi(z)\\) to predict mixture proportions using \\(q_i\\).\n",
    "        \"\"\"\n",
    "        self.pi_net.train()\n",
    "        q_tensor = torch.tensor(q, dtype=torch.float32)\n",
    "        for _ in range(100):  # Fixed number of optimization steps\n",
    "            self.optimizer.zero_grad()\n",
    "            pi_z = self.pi_net(self.z).squeeze()\n",
    "            loss = nn.BCELoss()(pi_z, q_tensor)  # Binary cross-entropy loss\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Run the EM algorithm until convergence.\n",
    "\n",
    "        Returns:\n",
    "        - mu: Estimated mean of the Gaussian component\n",
    "        - tau_squared: Estimated variance of the Gaussian component\n",
    "        - pi_net: Trained neural network for \\(\\pi(z)\\)\n",
    "        \"\"\"\n",
    "        for iteration in range(self.max_iter):\n",
    "            # E-step\n",
    "            q, pi_z = self.e_step()\n",
    "\n",
    "            # Save old parameters to check for convergence\n",
    "            old_params = np.array([self.mu, self.tau_squared])\n",
    "\n",
    "            # M-step\n",
    "            self.m_step(q)\n",
    "            self.update_pi_net(q)  # Update the neural network for \\(\\pi(z)\\)\n",
    "\n",
    "            # Check convergence\n",
    "            new_params = np.array([self.mu, self.tau_squared])\n",
    "            if np.linalg.norm(new_params - old_params) < self.tol:\n",
    "                print(f\"Converged in {iteration + 1} iterations.\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"Maximum iterations reached without convergence.\")\n",
    "\n",
    "        return self.mu, self.tau_squared, self.pi_net\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data\n",
    "    np.random.seed(42)\n",
    "    point_mass = np.zeros(100)\n",
    "    gaussian_component = np.random.normal(5, 2, 150)\n",
    "    x = np.concatenate([point_mass, gaussian_component])\n",
    "    z = np.random.uniform(-1, 1, len(x))  # Covariate\n",
    "\n",
    "    # EM algorithm parameters\n",
    "    sigma_0 = 1.0  # Fixed standard deviation for the point mass\n",
    "    tau_squared = 2.0  # Initial guess for variance of the Gaussian component\n",
    "\n",
    "    # Run the EM algorithm\n",
    "    em = EMAlgorithmWithCovariates(x, z, sigma_0, tau_squared)\n",
    "    mu, tau_squared, pi_net = em.run()\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Estimated \\u03bc: {mu}\")\n",
    "    print(f\"Estimated \\u03c4^2: {tau_squared}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cebmf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
