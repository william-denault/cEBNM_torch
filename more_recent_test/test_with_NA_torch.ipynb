{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76e6c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from cebmf_torch import *\n",
    " \n",
    "rng = np.random.default_rng(42)\n",
    "n,p=50,40\n",
    "u = rng.random(n); v=rng.random(p)\n",
    "X = np.outer(u,v) + rng.normal(0,0.1,(n,p))\n",
    "X[1,1]=np.nan; X[10,5]=np.nan; X[30:33,12:14]=np.nan\n",
    "Y = torch.tensor(X, dtype=torch.float32)\n",
    "m = cEBMF(Y, K=5, prior_L=\"norm\", prior_F=\"norm\")\n",
    "m.initialize(\"svd\")\n",
    "\n",
    "    # Ensure no nans in learned factors and tau positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2653e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "  mask_f = self.mask if self.mask.dtype.is_floating_point else self.mask.to(self.L.dtype)\n",
    "\n",
    "        Lk  = self.L[:, k]    # (N,)\n",
    "        Fk  = self.F[:, k]    # (P,)\n",
    "        Lk2 = self.L2[:, k]   # (N,)\n",
    "        Fk2 = self.F2[:, k]   # (P,)\n",
    "\n",
    "        # ---- 1) Add back k's contribution: R <- R + Lk Fk^T  (no outer allocation) ----\n",
    "        # This forms the partial residual that keeps all components except k removed.\n",
    "        self.R.addr_(Lk, Fk, alpha=1.0)\n",
    "        self.R.mul_(mask_f)\n",
    "        with torch.no_grad():  # ---------- Update L[:, k] ----------\n",
    "            if tau_map is None:\n",
    "                denom_l = mask_f @ Fk2                             # (N,)\n",
    "                # num_l[i]   = sum_j R[i,j] * Fk[j]\n",
    "                num_l   = self.R @ Fk                              # (N,)\n",
    "                se_l    = torch.sqrt(1.0 / (self.tau * denom_l.clamp_min(eps)))\n",
    "            else:\n",
    "                # denom_l[i] = sum_j tau_map[i,j]*mask[i,j]*Fk2[j]\n",
    "                denom_l = (tau_map * mask_f) @ Fk2                 # (N,)\n",
    "            # num_l[i]   = sum_j tau_map[i,j]*R[i,j]*Fk[j]   (no (tau_map*R) buffer)\n",
    "                num_l   = torch.einsum('ij,ij,j->i', self.R, tau_map, Fk)  # (N,)\n",
    "                se_l    = torch.sqrt(1.0 / denom_l.clamp_min(eps))\n",
    "\n",
    "            lhat = num_l / denom_l\n",
    "        # print(denom_l)\n",
    "\n",
    "        X_model = self.update_cov_L(k)\n",
    "        with torch.enable_grad():\n",
    "            resL = self.prior_L_fn.fit(\n",
    "                X=X_model,\n",
    "                betahat=lhat,\n",
    "                sebetahat=se_l,\n",
    "                model_param=self.model_state_L[k],\n",
    "            )\n",
    "        with torch.no_grad():\n",
    "            self.model_state_L[k] = resL.model_param\n",
    "            self.L[:, k] = resL.post_mean\n",
    "            self.L2[:, k] = resL.post_mean2\n",
    "            nm_ll_L = normal_means_loglik(x=lhat, s=se_l, Et=resL.post_mean, Et2=resL.post_mean2)\n",
    "            self.kl_l[k] = torch.as_tensor((-resL.loss) - nm_ll_L, device=self.device)\n",
    "            self.pi0_L[k] = resL.pi0_null if hasattr(resL, \"pi0_null\") else None\n",
    "\n",
    "            # ---------- Update F[:, k] ----------\n",
    "            # ---- 2) Subtract UPDATED L_k: R <- R - Lk_new Fk^T ----\n",
    "            Lk = self.L[:, k]  # updated\n",
    "            lk2 = self.L2[:, k]\n",
    "            self.R.addr_(Lk, Fk, alpha=-1.0)\n",
    "            self.R.mul_(mask_f)\n",
    "\n",
    "    # ---- 3) Add back (updated L_k) again to prepare F update: R <- R + Lk_new Fk^T ----\n",
    "            self.R.addr_(Lk, Fk, alpha=1.0)\n",
    "            self.R.mul_(mask_f)\n",
    "          \n",
    "\n",
    "        with torch.no_grad():\n",
    "            if tau_map is None:\n",
    "                # denom_f[j] = sum_i mask[i,j] * Lk2[i]\n",
    "                denom_f = mask_f.T @ self.L2[:, k]                 # (P,)\n",
    "                # num_f[j]   = sum_i R[i,j] * Lk[i]\n",
    "                num_f   = self.R.T @ Lk                            # (P,)\n",
    "                se_f    = torch.sqrt(1.0 / (self.tau * denom_f.clamp_min(eps)))\n",
    "            else:\n",
    "                # denom_f[j] = sum_i tau_map[i,j]*mask[i,j]*Lk2[i]\n",
    "                denom_f = (tau_map * mask_f).transpose(0, 1) @ self.L2[:, k]  # (P,)\n",
    "            # num_f[j]   = sum_i tau_map[i,j]*R[i,j]*Lk[i]\n",
    "                num_f   = torch.einsum('ij,ij,i->j', self.R, tau_map, Lk)     # (P,)\n",
    "                se_f    = torch.sqrt(1.0 / denom_f.clamp_min(eps))\n",
    "\n",
    "            fhat = num_f / denom_f.clamp_min(eps)\n",
    "\n",
    "        X_model = self.update_cov_F(k)\n",
    "        with torch.enable_grad():\n",
    "            resF = self.prior_F_fn.fit(\n",
    "                X=X_model,\n",
    "                betahat=fhat,\n",
    "                sebetahat=se_f,\n",
    "                model_param=self.model_state_F[k],\n",
    "            )\n",
    "        with torch.no_grad():\n",
    "            self.model_state_F[k] = resF.model_param\n",
    "            self.F[:, k] = resF.post_mean\n",
    "            self.F2[:, k] = resF.post_mean2\n",
    "            # store as scalar on device; PriorResult.loss already = -log_lik\n",
    "            nm_ll_F = normal_means_loglik(x=fhat, s=se_f, Et=resF.post_mean, Et2=resF.post_mean2)\n",
    "            self.kl_f[k] = torch.as_tensor((-resF.loss) - nm_ll_F, device=self.device)\n",
    "            self.pi0_F[k] = resF.pi0_null if hasattr(resF, \"pi0_null\") else None\n",
    "\n",
    "            Fk = self.F[:, k]  # updated\n",
    "            self.R.addr_(Lk, Fk, alpha=-1.0)\n",
    "            self.R.mul_(mask_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "590bf9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isfinite(m.L).all() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac99a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.isfinite(m.F).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f148a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "float(m.tau) > 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cebmf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
