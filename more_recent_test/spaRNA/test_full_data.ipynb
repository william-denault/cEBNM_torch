{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de9e5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willi\\anaconda3\\envs\\cebmf_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from huggingface_hub import login, hf_hub_download\n",
    "from cebmf_torch import cEBMF\n",
    "from cebmf_torch.torch_main import ModelParams, NoiseParams, CovariateParams\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12e37c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data\n",
    "# images = images\n",
    "# locs = locations x,y\n",
    "# ged = gene expression \n",
    "\n",
    "pwd = r\"C:/Document/Serieux/Travail/Data_analysis_and_papers/chevrier/torch_data\"\n",
    "dir1 = os.path.join(pwd, 'gene_expression_dataset.pt')\n",
    "'gene_expression_dataset.pt'\n",
    "dir2 = os.path.join(pwd, 'subimg_coord_dataset.pt') \n",
    "\n",
    "ged = torch.load(dir1)\n",
    "scd = torch.load(dir2)\n",
    "images = []\n",
    "locs = []\n",
    "for img, loc in scd:\n",
    "    images.append(img)\n",
    "    locs.append(loc)\n",
    "\n",
    "images = torch.stack(images)\n",
    "locs = torch.stack(locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c79c2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'processor', 'model', and 'device' are already defined.\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "\n",
    "\n",
    "def get_clip_embedding(image_input, model, processor, device):\n",
    "    \"\"\"\n",
    "    Takes a file path, PIL.Image, or torch.Tensor and returns a 512-dim CLIP embedding.\n",
    "    - Handles grayscale [1,H,W] tensors by repeating channels → [3,H,W].\n",
    "    - Works on GPU without unnecessary CPU/NumPy conversions.\n",
    "    \"\"\"\n",
    "    # Case 1: file path\n",
    "    if isinstance(image_input, str):\n",
    "        image = Image.open(image_input).convert(\"RGB\")\n",
    "\n",
    "    # Case 2: PIL image\n",
    "    elif isinstance(image_input, Image.Image):\n",
    "        if image_input.mode != \"RGB\":\n",
    "            image = image_input.convert(\"RGB\")\n",
    "        else:\n",
    "            image = image_input\n",
    "\n",
    "    # Case 3: torch tensor\n",
    "    elif isinstance(image_input, torch.Tensor):\n",
    "        image = image_input\n",
    "        if image.ndim == 4:  # [B,C,H,W], reduce to batch size 1 if needed\n",
    "            if image.shape[0] == 1:\n",
    "                image = image.squeeze(0)\n",
    "            else:\n",
    "                raise ValueError(\"Batch tensors not supported here. Pass single image.\")\n",
    "        if image.ndim != 3:\n",
    "            raise ValueError(f\"Expected [C,H,W] tensor, got shape {image.shape}\")\n",
    "\n",
    "        # Grayscale → RGB by channel repeat\n",
    "        if image.shape[0] == 1:\n",
    "            image = image.repeat(3, 1, 1)\n",
    "\n",
    "        # Convert tensor to PIL (processor expects PIL or numpy),\n",
    "        # but keep data on CPU just before feeding\n",
    "        image = transforms.ToPILImage()(image.cpu())\n",
    "\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported input type: {type(image_input)}\")\n",
    "\n",
    "    # Process and get embedding\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        image_emb = model.get_image_features(**inputs)\n",
    "        image_emb = image_emb / image_emb.norm(p=2, dim=-1, keepdim=True)  # normalize\n",
    "\n",
    "    return image_emb.squeeze(0)  # shape: (512,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c891c61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willi\\anaconda3\\envs\\cebmf_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\willi\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch32. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained CLIP\n",
    "device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "clip_embedded_images = []\n",
    "for image in tqdm(images):\n",
    "    tempimage = image.squeeze(0).to(device)\n",
    "    feature_emb = get_clip_embedding(tempimage, clip_model, clip_processor, device) # Extracted features (torch.Tensor) with shape [1,1536]\n",
    "    clip_embedded_images.append(feature_emb)\n",
    "\n",
    "clip_embedded_images = torch.stack(clip_embedded_images)  # shape (N, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07015bee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cebmf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
